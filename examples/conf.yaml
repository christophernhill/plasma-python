#conf.py will parse the yaml and extract parameters based on what is specified

#will do stuff in fs_path / [username] / signal_data | shot_lists | processed shots, etc.

fs_path: '/tigress'
target: 'hinge' #'binary' #'maxhinge' 
num_gpus: 4

paths:
    signal_prepath: '/signal_data/' #/signal_data/jet/
    shot_list_dir: '/shot_lists/'
    tensorboard_save_path: '/Graph/'
    data: 'jet_data'
    specific_signals: [] #if left empty will use all valid signals defined on a machine. Only use if need a custom set

data:
    augment_how: 0 #if greater than 0 means augment at random selecting self.conf['data']['augment'] signals, -1 will augment deterministically from the signals_to_augment
    signals_to_augment: [] #['plasma current']
    cut_shot_ends: True
    T_min_warn: 30
    recompute: False
    recompute_normalization: False
    #specifies which of the signals in the signals_dirs order contains the plasma current info
    current_index: 0
    plotting: False
    #train/validate split
    #how many shots to use
    use_shots: 200000 #1000 #200000
    positive_example_penalty: 1.0 #by what factor to upweight positive examples?
    #normalization timescale
    dt: 0.001
    #maximum TTD considered
    T_max: 1000.0
    #The shortest works best so far: less overfitting. log TTd prediction also works well. 0.5 better than 0.2
    T_warning: 1.024 #1.024 #0.512 #0.25 #1.0 #1.0 #warning time in seconds
    current_thresh: 750000
    current_end_thresh: 10000
    #the characteristic decay length of the decaying moving average window
    window_decay: 2
    #the width of the actual window
    window_size: 10
    #TODO optimize
    normalizer: 'augvar'
    shallow_sample_prob: 0.1 #the fraction of samples with which to train the shallow model
    floatx: 'float64'

model:
    shallow: False
    #length of LSTM memory
    pred_length: 200
    pred_batch_size: 128
    #TODO optimize
    length: 128
    skip: 1
    #hidden layer size
    #TODO optimize  
    rnn_size: 200
    #size 100 slight overfitting, size 20 no overfitting. 200 is not better than 100. Prediction much better with size 100, size 20 cannot capture the data.
    rnn_type: 'LSTM'
    #TODO optimize
    rnn_layers: 2
    num_conv_filters: 10
    size_conv_filters: 3
    num_conv_layers: 2
    pool_size: 2
    dense_size: 200
    #have not found a difference yet
    optimizer: 'adam'
    clipnorm: 10.0
    regularization: 0.0
    dense_regularization: 0.01
    #1e-4 is too high, 5e-7 is too low. 5e-5 seems best at 256 batch size, full dataset and ~10 epochs, and lr decay of 0.90. 1e-4 also works well if we decay a lot (i.e ~0.7 or more)
    lr: 0.00001 #0.0005 #for adam plots 0.0000001 #0.00005 #0.00005 #0.00005
    lr_decay: 1.0 #0.98 #0.9
    stateful: True
    return_sequences: True
    dropout_prob: 0.3
    #only relevant if we want to do mpi training. The number of steps with a single replica
    warmup_steps: 0
    ignore_timesteps: 100 #how many initial timesteps to ignore during evaluation (to let the internal state settle)
    backend: 'tensorflow'
training:
    as_array_of_shots: True
    shuffle_training: True
    train_frac: 0.75
    validation_frac: 0.33
    batch_size: 256
    #THIS WAS THE CULPRIT FOR NO TRAINING! Lower than 1000 performs very poorly
    max_patch_length: 100000
    #How many shots are we loading at once?
    num_shots_at_once: 200
    num_epochs: 4
    use_mock_data: False
    data_parallel: False
    hyperparam_tuning: False
    batch_generator_warmup_steps: 0
    num_batches_minimum: 200 #minimum number of batches per epoch
    ranking_difficulty_fac: 1.0 #how much to upweight incorrectly classified shots during training
callbacks:
    list: ['earlystop']
    metrics: ['val_loss','val_roc','train_loss']
    mode: 'max'
    monitor: 'val_roc'
    patience: 4
    write_grads: False
